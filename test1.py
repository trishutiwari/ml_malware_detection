#I love Alex

# To run the code type the following into the command prompt 
# in the EC_521_ML folder type python test1.py. 
import keras as keras
import time
import datetime
from keras.models import Sequential
from keras.layers import Dense
import numpy as np 

seed = 7
np.random.seed(seed)

# *************************************************************************
#**************** This part of the code formats the data ******************
#**************************************************************************

# This is where we will load the data-set 
# Assuming we will load from csv... 
# dataset = np.loadtxt(...)
arrayDataLength = 133

theData = np.load("nnData.npy")
theLabels = np.load("nnLabels.npy")
classType = [0, 0, 0, 0]
for i in theLabels: 
	if i == 0:
		classType[0] += 1
	if i == 1: 
		classType[1] += 1
	if i == 2: 
		classType[2] += 1
	if i == 3:
		classType[3] += 1

print(classType[0])
print(classType[1])
print(classType[2])
print(theLabels[classType[0]+1])
zeus = theLabels[:classType[0]]
conficker = theLabels[classType[0]:classType[1]]
dridex = theLabels[classType[1]+classType[0]:classType[2] -1]

zeusD = theData[0:len(dridex)] 
confickerD = theData[classType[1]:len(dridex)]  
dridexD = theData[classType]
zeus = zeus[0:len(dridex)]
conficker = conficker[0:len(dridex)]
theData = np.concatenate((zeus))
# theData = np.concatenate(zeus, conficker) 

# theData = np.reshape(theData, (len(theLabels), arrayDataLength))
# Split the data set into training and testing here: 
# [[packet, packet, packet, packet, packet, packet label]
#  [ packet packet packet packet packet,    packet label]]
# trainData = theData[0:len(theData)-1000] # np.random.choice(theData, len(theData) -1000)
# count = 0
# trainData = np.array([])

# for data in theData:
#   trainData = np.append(trainData, np.append(data, theLabels[count]))
#   count += 1

# trainData = np.reshape(trainData, (len(theLabels), arrayDataLength+1))

# idxTrain = np.random.choice(len(trainData), len(trainData)-1000)
# idxTest = np.random.choice(len(trainData), 1000)


# testData = trainData[idxTest,:]
# trainData = trainData[idxTrain,:] 
# trainLabels = trainData[:, arrayDataLength:]
# trainLabels = keras.utils.to_categorical(trainLabels, num_classes=3)
# trainData = trainData[:,0:arrayDataLength]
# testLabels = testData[:,arrayDataLength:]
# testData = testData[:,0:arrayDataLength]

# #print(trainLabels)
# #test = dataset[:]

# #************************************************************************
# #************************************************************************

# #************************************************************************
# #************** This part of the code makes the CNN *********************
# #************************************************************************

# # Create the model here: 

# 	pass
# neuralNetModel = Sequential()
# numInputs = arrayDataLength 
# # neuralNetModel = load_model('weight1.h5')
# # This should create a sequential model that accepts ? inputs and
# # has a signle hidden layer with 1000 nodes. The dense command makes it a 
# # fully connected graph. We have 1 output node one for our first model
# # that does binary classification. 


# # ******************* Instructions ***************************
# # The following are the things you can tune within the model: 
# # nH1 is equal to the nodes in the hidden layer 1.
# #testNumber = 0 
# #activationList = ["sigmoid","relu","softmax"]
# nH1 = 0
# # logFile = open("Test_result2s.txt","a+")
# progStartTime = str(datetime.datetime.now())
# # logFile.write("Test started at %s \r\n" % progStartTime)
# # for activation_type in activationList:
# # 	logFile.write("\r\n")
# # 	logFile.write("------------------------------------------------------ \r\n")
# # 	logFile.write("Changing Activation to %s \r\n" % activation_type)
# # 	logFile.write("------------------------------------------------------ \r\n")
# # 	logFile.write("\r\n")
# startTime = time.time()
# # Too many nodes will overfit the training set. Too little nodes and you wont 
# # be capturing all the features. 
# # The more nodes the longer it will take to run. So be forewarned. 
# # ****************** End Instruction ****************************
# # neuralNetModel.add(Dense(nH1, input_dim=numInputs, activation='relu')) # Tested different activation function
# neuralNetModel = Sequential()
# numInputs = arrayDataLength 
# activation_func = "sigmoid"
# neuralNetModel.add(Dense(3000,6666 input_dim=numInputs, activation=activation_func))
# # neuralNetModel.add(Dense(nH1, input_dim=numInputs, activation='softmax')) # Tested different activation function
# # 

# neuralNetModel.add(Dense(4, activation=activation_func)) 
# 	# This command compiles the model
# neuralNetModel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])


# # ***************** Instruction ************************** 
# # The following are the things you can change in the neural net
# # The number of Epochs of training. The more epochs the more you overfit the 
# # the training set. Too little epochs and you will not capture enough features. 
# # sizeBatch is how many training values does the model train on at once. 
# # the batch size is always less then the actual number of data. The smaller the 
# # batchSize the fewer features you are training against at once. Speeds up or slows down 
# # your model 
# numEpochs = 20
# sizeBatch = 100
# # ***************** End Instruction **************************
# # The following command will train the model on some data: 
# neuralNetModel.fit(trainData, trainLabels, epochs=numEpochs, batch_size=sizeBatch)

# # Find predictions on some test set 
# # predictions = model.predict(test) 
# endTime = time.time()
# # ******************* Instruction **************************************
# # The metric you are going to try and increase is accuracy. 
# # If you can get it to 60 or 70 then we have succeeded and just save the weights. 
# # ******************** End Instruction ********************************
# scores = neuralNetModel.evaluate(trainData, trainLabels)

# print("\n%s: %.2f%%" % (neuralNetModel.metrics_names[1], scores[1]*100))
# now = datetime.datetime.now()
# filename ="Weights_"+ activation_func+(now.strftime("_%Y_%m_%d_%H_%M"))+".h5"
# print("Saving Weights: ")
# neuralNetModel.save_weights(filename)
# print("Saved Weights")



# progEndTime = (datetime.datetime.now())
# progRunTime = endTime-startTime
# print("Test Run Time was %s \r\n" %progRunTime) 



	