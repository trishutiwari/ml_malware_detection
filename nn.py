#!/usr/bin/env python3
import warnings
warnings.filterwarnings("ignore")
import numpy as np
np.random.seed(0)  # for reproducibility
 
from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation, Flatten
from keras.layers import Conv2D, MaxPooling2D
from keras.utils import np_utils
# from scapy.all import *
from pcap_reader import SessionObjectOrder, PcapReader
from datasketch.minhash import MinHash
from keras.models import load_model

# def get_payload(pkt):
# 	# gets the payload of the highest network layer in a packet
# 	while pkt.payload:
# 		pkt = pkt.payload
# 	return str(pkt)

# def get_packet_seq_min_hash(packets,packet_range_begin,step):
# 	# returns the combined locality sensitive hash of 
# 	# the 100 packets given as an argument (and so step = 100)

# 	# each hash is a 1D array of 128 integers
# 	packet_seq_min_hash = MinHash()
# 	for packet_index in range(packet_range_begin,packet_range_begin+step):
# 		try:
# 			payload = get_payload(packets[packet_index])
# 		except IndexError:
# 			break
# 		packet_seq_min_hash.update(payload)
# 	return packet_seq_min_hash.hashvalues

# def get_pcap_min_hash(file,step):
# 	# returns a list of hashes for the entire pcap file
# 	hash_array = list()
# 	packets = rdpcap(file)
# 	for packet_range_begin in range(0,len(packets),step):
# 		payload_min_hash = get_packet_seq_min_hash(packets,packet_range_begin,step)
# 		hash_array.append(payload_min_hash)
# 	return hash_array

# def get_malware_hash(malware,range_max,step):
# 	# returns a list of hashes for an entire malware type 
# 	# (each malware is distributed over multiple pcaps)
# 	X = list()
# 	for i in range(1,range_max):
# 		print malware, i
# 		file = "./data/{0}0{1}.pcap".format(malware,i)
# 		X.append(get_pcap_min_hash(file,step))
# 	print np.concatenate(X).shape
# 	return list(np.concatenate(X))

def cnn(train_data,train_classes,test_data,test_classes,num_classes):
	# the CNN model

	# input shape: 
		# depth = 1
		# each input is a hash, and each hash is an array of 128 numbers,
		# so the dimensions of each individual input is as follows:
		# rows = 128 
		# cols = 1
		# hence, input_shape = (1,128,1)

	model = Sequential()
	model.add(Conv2D(32, (1, 1), activation="relu", input_shape=(1, 128, 1)))
	model.add(Conv2D(32, 1, 1, activation='relu'))
	model.add(MaxPooling2D(pool_size=(1,1)))
	model.add(Dropout(0.25))
	model.add(Flatten())
	model.add(Dense(128, activation='relu'))
	model.compile(loss='categorical_crossentropy',
	              optimizer='adam',
	              metrics=['accuracy'])
	model.fit(train_data, train_classes,epochs=10, verbose=1)
	score = model.evaluate(test_data, test_classes, verbose=0)
	return score

def nn(train_data,train_classes,test_data,test_classes,num_classes):
	# the regular NN model
	# TO DO: ADD A MODEL THAT WORKS WITH GIVEN DATA
	model = Sequential()
	model.add(Dense(128, activation='relu',input_shape=(1, 128, 1)))
	model.compile(loss='categorical_crossentropy',
	              optimizer='adam',
	              metrics=['accuracy'])
	model.fit(train_data, train_classes,epochs=10, verbose=1)
	score = model.evaluate(test_data, test_classes, verbose=0)
	return score


def retrieve_datasets(malware_name_list,end_index_list):
	# retrives the hashes for all malware and compiles it into an array of training_data
	# and an array of labels.
	class_counter = 0
	train_data = [[0]*133] 
	train_classes = list()
	for malware_name,end_index in zip(malware_name_list,end_index_list):
		reader = PcapReader(malware_name,end_index)
		malware_data = reader.read_malware()
		print(len(malware_data))
		for malSamp in malware_data: 
			# print(malSamp)
			train_data = np.concatenate((train_data,[malSamp]), axis=0)
			train_classes += [class_counter]
		class_counter+=1
	train_data = train_data[1:]
	# print(train_classes)
	return train_data,train_classes

# def split_train_test(train_data,train_classes):
# 	# 80% data for training, 20% for testing
# 	split_percentage = 0.80
# 	split_index = int(len(train_data)*split_percentage)
# 	# shuffle the data
# 	tmp = list(zip(train_data,train_classes))
# 	np.random.shuffle(tmp)
# 	train_data,train_classes = zip(*tmp)
# 	# split into train and test sets
# 	train_data = train_data[:split_index]
# 	test_data = train_data[split_index:]
# 	train_classes = train_classes[:split_index]
# 	test_classes = train_classes[split_index:]
# 	return train_data,train_classes,test_data,test_classes

def format_input_nn(train_data,train_classes):#,test_data,test_classes):
	# reshapes the data so that we have the following format
	# (num samples, depth, first dimension of input, second dimension of input)
	train_data = np.array(train_data)
	# train_data = train_data.reshape(train_data.shape[0],1,133,1)
	# train_data = train_data.astype('float32')
	# test_data = np.array(test_data)
	# test_data = test_data.reshape(test_data.shape[0],1,133,1)
	# test_data = test_data.astype('float32')
	train_classes = np.array(train_classes)
	tmp = list(zip(train_data,train_classes))
	np.random.shuffle(tmp)
	train_data,train_classes = zip(*tmp)
	# train_classes = np_utils.to_categorical(train_classes, len(malware_name_list))
	# test_classes = np.array(test_classes)
	# test_classes = np_utils.to_categorical(test_classes, len(malware_name_list))
	return train_data,train_classes#,test_data,test_classes

def format_input_cnn(train_data,train_classes):
	train_data = np.concatenate(train_data)
	return train_data,train_classes

malware_name_list = ['zeus','conficker','dridex', 'trishita']
end_index_list = [4,2,2,1]

if __name__ == "__main__":
	train_data,train_classes = retrieve_datasets(malware_name_list,end_index_list)
	# put the next lines inside range(k) forloop for k fold cross validation
	# train_data,train_classes,test_data,test_classes = split_train_test(train_data[1:],train_classes)
	train_data,train_classes = format_input_nn(train_data,train_classes)
	np.save("nnData", train_data)
	np.save("nnLabels", train_classes)
	print(train_data[:2],train_classes[:2])
	train_data,train_classes = format_input_cnn(train_data,train_classes)
	print(train_data[:2],train_classes[:2])
	np.save("cnnData", train_data)
	np.save("cnnLabels", train_classes) 
	# score = nn(train_data,train_classes,test_data,test_classes,len(malware_name_list))
	# print "score:",score



